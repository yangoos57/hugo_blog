---
title: "정보 엔트로피"
date: 2022-03-06T13:47:07+09:00
image: "images/home/entropy.png"
Tags: ['엔트로피','Entropy']
draft: true
---
<br>

정보 Entropy는 집단 내 요소의 확률 분포를 알고 있는 상태에서, 새로운 요소를 예측하기 위해 필요한 질문 개수를 알려준다.

<br>


**정보 Entropy 공식**

<br>

{{< center >}}$H \ = \ E(x) = \  -\sum p(x) · \log_{2} p(x) $
{{< /center >}}

<br><br>

처음 정보 Entropy 공식을 접하면 엔트로피라는 의미와 $\log$로 인해 생소하고 심오해 보인다. 하지만 이 공식은 사실 고등학생때 배운 확률과 통계에서 봤던 기댓값 공식과 다르지 않다. 

이 공식은 무엇을 의미하는걸까? 상당히 단순한 이 공식은 여러 관점에서 다양하게 해석 가능하다. 자연과학 용어인 엔트로피와 공식이 같기에 정보 엔트로피라고 이름 불리는 것만봐도 단순한 공식이 세상 돌아가는 여러가지 현상을 설명할 수 있는 단순하지만 강력한 공식이라는 것을 알 수 있다.

정보 엔트로피에서도 해당 공식을 어떻게 바라보냐에 따라 해석을 달리할 수 있다. 그 예시로 정보량을 Surprise로 이해하는 방식이다. 정보 entropy를 정보량 = Surprise라는 개념으로 설명하는 유튜브가 있다. Surprise라는 개념을 알고싶거나 이를 활용해 정보 entropy를 이해하고 싶다면 추천한다. 

이 글에서는 정보량을 대상을 구분하기 위해 필요한 질문 개수라는 직관적인 개념을 바탕으로 정보 Entropy를 설명하고자 한다. 


<br>

## 기댓값 | `Expected Value`

기댓값은 확률변수가 취할 수 있는 모든 값들의 평균이다. 우리가 특정 집단의 이산확률분포를 알고 있는 상황이라면 집단의 기댓값을 구할 수 있다. 

<br>

![확률분포](/statistics/basic/dice.png) 
출처 위키피디아 이산확률분포

<br>

위 그림은 주사위 2개를 던져 나온 값의 합계의 이산확률분포이다. 차트를 보면 주사위 합이 7이 될 확률은 6/36이라는 것을 알 수 있다. 

<br>

**기댓값 공식**

해당 이산확률분포의 기댓값은 2개의 주사위를 던져 나온 값들의 평균을 의미한다.

<br>

{{< center >}} $E(x) = \sum \ p(x) \ ·x \quad \quad $ {{< /center >}} 

<br><br>

**주사위 2개 합에 대한 기댓값**

주사위를 무한히 던진다면  그 값이 평규적으로 `7`에 수렴한다는 것을 알 수 있다. 

<br>

 {{< center >}} $E(x) =  2·\frac{1}{36} + 3·\frac{2}{36} + 4·\frac{3}{36} + \ \cdots \ + 11·\frac{2}{36} + 12·\frac{1}{36} = \frac{252}{36}  \ = \ 7 $
{{< /center >}}
<br><br>


## 기댓값과 정보 엔트로피

기댓값과 정보 엔트로피 공식은 같다. 보다 직관적인 설명을 위해 정보 엔트로피 공식을 변형해보자. $\sum$ 앞에 있는 마이너스( - )를 $log_{2}{p(x)}$ 앞으로 옮기자.그러면 $ -\log_{2}{p(x)}$ 을 $ \log_{2}({\frac{1}{p(x)}}) $으로 변환된다.

<br>

**수정 된 엔트로피 공식**

<br>

{{< center >}}$ H \ = \ -\sum p(x)\ · \log_{2}{p(x)} \ = \ \sum p(x)\ · \log_{2}({\frac{1}{p(x)}})  ${{< /center >}}

<br><br>

다시 $ \log_{2}({\frac{1}{p(x)}}) $를 $x$로 치환하면
$E(x) = {\sum} \  { p(x) \ · x}$가 된다. 기댓값 공식과 변형 된 정보 엔트로피 공식이 같다. 즉 정보 엔트로피는 $x$라는 값에 대한 기댓값으로 이해할 수 있다.

<br>

이제 $\log_{2}({\frac{1}{p(x)}})$를 이해한다면 Entropy가 무엇에 대한 기댓값인지 이해할 수 있다.




<br>

## 정보량
$\log_{2}({\frac{1}{p(x)}})$은 정보량의 공식이다. 그러므로 정보량이 무엇인지, 정보량의 공식인 $\log_{2}({\frac{1}{p(x)}})$은 어떻게 만들어졌는지 이해한다면 정보 엔트로피가 무엇을 의미하는지 알 수 있다.

<!-- 처음 정보량이라는 용어를 접한다면 무의식 중으로 정보량이 많은게 좋다는 인식을 갖는다. 아는 것이 힘이라는 말이 있듯 정보를 많이 안다면 좋은 것이라는 인식이 있기 때문이다. 최근 우크라이나 러시아 전쟁을 보더라도 미국의 막대한 정보력이 우크라이나를 살리고 있다는 평이 나올정도로 정보는 중요하고 정보량이 많으면 좋다는 인식이 있다. 

하지만 정보량이 많으면 긍정적이라는 기본 인식은 정보량(Information)이라는 개념을 이해하는데 오히려 방해가 된다. 

개인적인 이야기로 바꿔쓰는것도 나쁘지 않을듯 
5년 동안 정보장교로 근무하면서 생긴 인식, 정보가 많다는게 좋다는 인식, 오히려 정보량을 이해하는데 방해가 됐다. -->

우리가 배우고 있는 정보량, 정보엔트로피는 통신을 위한 목적으로 발전된 학문이다. 다들 알다싶이 정보량이라는 개념은 1920년대 Hartl?이 최초로 표현했고 1940년도 클러드 섀넌이라는 천재가 이를 정보이론으로 발전시켰다. 

클러드 새넌이 관심가진 것은 어떻게 하면 제한된 용량에서 최대한 많은 내용을 보내느냐는 것이었다. 예를 들어보자. A와 B가 1~8 사이 숫자 3개를 맞추는 게임을 하고 있다. 숫자는 랜덤으로 생성되며  A는 답을 알고 있다. B는 A에게 질문을 해서 답이 무엇인지를 알아내야 한다. 이때 A는 Yes or No로만 대답해야한다. B가 숫자를 맞추기 위해서 택해야 하는 최선의 전략은 무엇일까?

단순히 숫자가 1인지, 2인지 ... 8인지 순차적으로 물어보는 전략을 택한다면 어떻게 될까? 운이 좋다면 한 번에 맞출 수 있지만 운이 좋지 않다면 최대 8번만에 맞출 수 있다. 모 아니면 도 전략이다. 

![onetwo](/statistics/basic/onetwo.png)

여기서 B가 택해야 하는 전략은 이진탐색이다. 이진탐색 은 선택지를 1/2씩 줄여나가는 방법이다. 1~8의 중앙값(median) 4를 기준으로 정답이 4보다 큰지, 4보다 작은지를 A에게 물어보는 것이다. 이런 방식은 선택지를 절반씩 줄여나가기 때문에 매우 효과적으로 대상을 줄여나갈 수 있다.

<그림>

위 그림은 이진 탐색 전략을 택할 때 문제의 정답에 따라 필요한 질문 수를 보여준다. 답이 어떤수인지 상관 없이 B는 3번만 물어보면 모든 문제를 해결 할 수 있다. 

이번에는 0~9 중 하나를 맞추는 경우라 생각해보자. 0~9를 맞출 때는 3~4번 사이면 대부분 해결된다. 그중 3번만에 정답을 맞출 확률이 4번만에 맞출 확률보다 월등히 높다. 이러한 비율은 0~9 문제를 평균 3.x번 질문만에 맞출 수 있다고 생각할 수 있다.

마지막으로 대문자 알파벳을 맞추는 문제를 생각해보자. 알파벳은 A~Z까지 총 26개가 있다. 5번만에 맞출 경우는 20번 4번만에 맞출 경우는 총 6번으로 평균 4.76번이면 맞출 수 있다. 

B가 맞춰야 하는 값의 범위가 8-> 10 -> 26개로 점점 커졌음에도 불구하고 B가 물어봐야 하는 질문 개수는 3개 -> 3.x개 -> 4.76개로 그다지 늘지 않는다. 이 말은 이진탐색 전략이 상당히 효과적인 전략이라는 점을 알 수 있다.

가장 효율적인 전략인 이진탐색은 대상을 1/2씩 줄여나간다. 범위가 8개인 대상을 4개로 2개로 1개로 줄여나간다. 16개인 대상은 8개 4개 2개 1개로 줄여나간다. 

<8과 16 그림>

우리가 여기서 알 수 있는 사실은 $2^{평균 질문개수} = 문제범위$ 라는 것이다. 8과 16 범위에 따라 질문 3번만에 답을 구하는 확률과 질문 4번만에 답을 구하는 확률이 달라짐을 알 수 있다. 8에서 16으로 증가함에 따라 필요한 평균 질문 개수도 증가한다. 이는 당연히 3번 질문해야하는 경우는 8개에서 0개로 감소하는 반면 4번 질문해야하는 경우는 0개에서 16개로 증가하기 때문이다. 

&2^평균질문개수 = 문제범위$가 성립됨을 경험적으로 알게 됐으니 공식을 약간 수정해보자. 우리가 문제범위를 알고 있다면 하나를 맞추기 위해 필요한 평균 질문개수가 몇개인지를 알 수 있다. 

${평균질문개수} = log_{2}{문제범위}$ 식은 문제범위가 몇개인지 안다면 평균질문개수가 어떻게 될지 쉽게 알 수 있다. 여기서 놓치지 말아야할 사실은 평균 질문 개수는 대상 하나를 식별하기 위해 필요한 평균 질문 수를 의미한다. 

알파벳 하나를 식별하기 위해서는 $log_{2}{26} \approx 4.7004$개의 질문이 필요하다. 알파벳 10,000개를 알아내야한다면 4.7004*10000이 되므로 47004번 질문하면 된다.

이를 데이터 단위로 표현하면 알파벳 10,000개를 식별하기 위해서는 47004비트만 있으면 된다는 말이기도 하다.


<br>

**$log_{2}$문제범위 로 평균 질문 개수를 구할 수 있는지 검증하기**

사실 지금 설명하고자 하는 내용이 내가 엔트로피에 대한 글을 작성하게 된 동기였다. 어떻게 $log_{2}{문제범위}$라는 식이 평균 질문개수를 바로 계산할 수 있는지 의문이 들었다. 

이런 의문을 해결하고자, 알파벳을 맞추기 위해 필요한 질문 개수가  $log_{2}{문제범위}$로 얻은 값과 아래 그림처럼 질문 개수와 확률을 통해 얻은 값이 같은지 확인했다. 

그 결과 $log_{2}(26)$는 4.7004이고 4와 5가 나오는 비율로 계산한 결과는 4.76이었다. 이 두 값은 유사하다고 할 수 있지만 알파벳 1000개를 알아내는 질문인 경우 약 500여개 정도 차이나기 때문에 우연의 일치가 아닌가 하는 의문이 생겼다. 한편으로는 내가 정보엔트로피를 제대로 이해하고 있는게 맞나 싶은 강력한 의문을 가지게 됐다. 

결론부터 말하면 문제 푸는 횟수가 증가 할수록 $log_{2}(26)$ 값에 수렴한다.

아래 코드를 활용해 알파벳을 100만번 알아내는 경우를 실험해봤다. 횟수가 증가할수록 알파벳 하나를 알아내기 위해 필요한 질문 개수가 4.7004에 수렴하는 것을 볼 수 있다.

<br>

![output](statistics/basic/output.png)
{{< center >}} Alphabet문제를 1,000,000번 실행한 결과{{< /center >}}

<br>

나와 같은 궁금함을 가진 사람들을 위해 작성한 코드를 공개한다. 

<br>

```python
import math
import pandas as pd
import numpy as np

k = []

for j in range(1000000) :  
    alpha = list(range(26)) ## Alphabet을 0~25로 치환한다고 생각
    target = np.random.randint(26)  ## 0~25 중 랜덤으로 값 생성

    i = 0 
    while i < 10 :
        median = (alpha[0] + alpha[-1])// 2

        if len(alpha) == 2 or(median == target and len(alpha) == 3):
            k.append(i+1) ## i = 물어본 질문 수 
            break
        
        ## 값이 짝수개인 리스트를 나눌 때
        elif len(alpha)%2 == 0 :
            if target > median :
                alpha = list(range(median+1, alpha[-1]+1))
                # print(f" 짝 T >= M --- median : {median} --- alpha : {alpha}")  

            if target <= median : 
                alpha = list(range(alpha[0], median+1))
                # print(f" 짝 T < M --- median : {median} --- alpha : {alpha}")

        ## 값이 홀수개인 리스트를 나눌 때
        elif len(alpha)%2 == 1 : 
            if target >= median :
                alpha = list(range(median, alpha[-1]+1))
                # print(f" 홀 T > M --- median : {median} --- alpha : {alpha}")

            if target < median : 
                alpha = list(range(alpha[0], median))
                # print(f" 홀 T < M --- median : {median} --- alpha : {alpha}")
                
        i += 1

result = pd.Series(k)
mean = result.mean()  ## 평균
val_k = result.value_counts() ## 빈도수

print('평균 : ', round(mean,6))
print('log_2(26) :', round(math.log2(26),6))
print('차이 :', round(mean - math.log2(26),6))
print('-'*10)
print('-'*10)
print(val_k)
```

<br><br>

**정보량과 평균질문개수**

이 글의 목적이 엔트로피 공식을 이해하는 것이므로 지금까지 배운 내용을 다시 정리하여 어떻게 엔트로피와 관련이 있는지 알아보겠다. 우리는 정보 엔트로피가 X에 대한 기대값이라는 사실을 알았다. X라 함은 $\log_{2}({\frac{1}{p(x)}})$ 공식을 말한다. 그리고 $\log_{2}({\frac{1}{p(x)}})$ 공식은 정보량이라는 이름을 가지고 있다. 우리가 1~8, 0~9, 알파벳 문제를 풀면서 알게된 공식은 평균질문개수 = $log_{2}$문제범위 였다. 

정보량이라는 개념은 확률을 통해 구한 값인 반면 평균질문 개수는 문제범위를 가지고 구한 값이므로 같은 두 공식이 닮긴 했는데 같인 값인지 애매하다. 이 내용의 처음으로 다시 돌아가보자. A와 B는 랜덤으로 생성된 값을 가지고 게임을 하고 있다. A는 알파벳 A~Z 중 랜덤으로 하나의 값을 고르고 B는 질문을 통해 값을 맞춰야한다. 

여기서 우리는 너무 당연히 여기다보니 고민조차 하지 않는 조건이 하나 있다. 다른 조건이 주어지지 않는 한 A~Z를 무작위로 무수히 선택한다면 개별 값의 빈도수가 모두 같으리라 생각한다. 이를 다르게 표현하자면 개별 값들이 선택될 확률이 일정하다는 사실이다. 즉 A-Z의 개별 값은 1/26의 확률을 가진다.

테이블

|값|확률|
|:------:|:---:|
|A|$\frac{1}{26}$|
|B|$\frac{1}{26}$|
|$\vdots$|$\vdots$|
|Y|$\frac{1}{26}$|
|Z|$\frac{1}{26}$|

A~Z까지 확률이 같으니 대표로 A의 정보량을 구해보자.  $p(x)=\frac{1}{26}$ 이므로 $\log_{2}({\frac{1}{\frac{1}{26}}})$ = $log_{2}(26)$ 이 된다.

흥미롭게도 정보량과 평균질문개수가 동일한 식이다. 따라서 정보량이라는 이름을 가진  $\log_{2}({\frac{1}{p(x)}})$은 대상 하나를 식별하는데 필요한 평균질문개수를 의미한다. 

<br><br>


## 엔트로피 공식 이해하기
지금까지 엔트로피 공식이 X에 대한 기댓값이라는 사실과 기댓값 X로 치환했던 존재인 $\log_{2}({\frac{1}{p(x)}})$가 평균질문개수라는 사실을 설명했다. 

이렇게 알게 된 두 사실을 조합하면 정보 엔트로피 공식의 의미를 알 수 있다. 결론을 도출하기에 앞서 앞에서 계속해서 설명한 예시인 알파벳 문제의 정보 엔트로피를 구해보자.

<br>

**정보 Entropy 공식**

{{< center >}}$ H \ = \ -\sum p(x)\ · \log_{2}{p(x)} \ = \ \sum p(x)\ · \log_{2}({\frac{1}{p(x)}})  ${{< /center >}}

<br><br>

A~Z는 발생할 확률이 동일하다. 확률 $p(x)=\frac{1}{26}$ 이고 정보량 $I(x) = log_{2}(26)$이다. 따라서 $ H = E(x) = \sum \frac{1}{26} \ · \ log_{2}(26) $이다.  

A~Z 값이 26개이므로 아래와 같은 식이 성립된다. 
<br>

$\frac{1}{26} \ · \ log_{2}(26) · 26 = log_{2}(26) \approx 4.70004$

정보 엔트로피를 계산하니 정보량과 값이 같다. 돌고돌아보니 결국 정보 엔트로피, 정보량 평균 질문 개수는 동의어에 불과하다..... 과연 그럴까?

<br><br>

### 발생할 확률이 동일하지 않은 경우
지금까지 우리는 발생할 확률이 동일한 경우를 예시로 활용했다. 예시가 다르다고 지금까지 배웠던 개념이 달라지는건 없다. 정보 엔트로피는 X에 대한 기대값이고 정보량인 X는 평균 질문개수다.

<!-- 
좀 더 현실적인 예시를 들어보자. 아래 테이블은 전세계 노트북 시장 점유율을 보여준다. 주어진 값은 상위 5개 기업과 나머지 기업을 합친 others로 총 6개이다. A와 B가 이번엔 노트북 회사 이름 맞추기 게임을 하고 있다.. A는 기업의 점유율에 맞게 회사를 선택한다. B는 문제 하나를 맞추기 위해 평균 몇 번을 질문 해야할까? 

|회사|점유율|
|:------:|:---:|
|Lenovo|$30%$|
|HP|$20%$|
|DELL|$15%$|
|APPLE|$10%$|
|ACER|$5%$|
|Others|$20%$|

이전 게임에서 최고의 전략으로 여겨졌던 이진탐색을 사용해보자.

정보량은 구할 수 있는데 트리는 어떻게 구현해야하는지 모르겠다. -->


이번엔 난이도를 높혀서 A~D의 발생확률이 다른 상황에서 문제를 맞춰야 한다. A~D가 발생할 확률은 아래 테이블과 같다. 

|값|확률|
|:------:|:---:|
|A|$50%$|
|B|$25%$|
|C|$12.5%$|
|D|$12.5%$|

앞선 예시처럼 단순히 절반을 나누는 방법을 사용해보자. 아래 그림과 같은 구조를 가지게 되며 정보엔트로피 = 정보량 = 평균 질문 개수 = 2가 된다.
그림

하지만 개별 값의 확률 분포를 고려하지 않고 만들었기에 최적의 전략은 아니다.
차라리 이런 모양의 트리를 만든다면 어떻게 될까? A가 선택될 학률이 50%이니 가장 먼저 A를 물어보는게 가능성을 더 높혀주기 때문이다. 이와 마찬가지로 B가 C 또는 D보다 발생 확률이 2배 높으니 B를 물어보는게 좋은 선택이 된다. 완성된 트리는 다소 불균형해 보이지만 실제로는 효과가 좋다. 


이 전략이 효과적이라는 사실을 알았다. 이제는 B가 이 방법을 택해서 문제를 해결한다면 평균 몇번 질문을 필요로하는지 계산해보자. 단순히 반반씩 나눈 전략에 비해 얼마나 질문 수를 줄일 수 있을지 비교하기 위함이다.

A와 B는 40번의 문제 맞추기 게임을 했다. 문제 비율은 확률과 똑같이 발생했다. A는 40번 중 20번, B는 10번 C와 D는 각각 5번 발생했다.  


|값|빈도수|질문 개수|
|:--:|:----:|:--:|
|A|20회|1번|
|B|10회|2번|
|C|5회|3번|
|D|5회|3번|
총합|40회||

빈도수와 질문 개수를 알았으니 이 방법을 사용한다면 평균적으로 사용하는 질문 개수를 알 수 있다. 이 방법을 활용하면 평균 1.75번 질문이 필요하다는 사실을 알수 있다.

<br>

{{< center >}}$\frac{20·1+10·2+5·3+5·3}{40} \ = \ \frac{20}{40} · 1 + \frac{10}{40} · 2  + \frac{5}{40} · 3 + \frac{5}{40} · 3 \ = \ 1.75$ {{< /center >}}

<br><br>




### 정보 엔트로피와 평균 질문 개수
앞선 설명에서 알파벳 문제 처럼 개별 값들의 확률이 동일할 경우, 정보 엔트로피 = 개별 값의 정보량 = 평균 질문 개수라는 사실을 확인했다. 개별 값이 발생할 확률이 다른 경우에도 정보 엔트로피 = 개별 값의 정보량 = 평균 질문 개수가 사실일까? 발생 확률이 다른 A~D 문제의 정보 엔트로피를 구해보자.

<br>

**정보 엔트로피 공식**

|값|$p(x)$|$\log_{2}{p(x)}$|
|:--:|:----:|:-----------------------:|
|A|$\frac{1}{2}$|$\log_{2}{\frac{1}{\frac{1}{2}}} \ = \ \log_{2}{2}$ = 1|
|B|$\frac{1}{4}$|$\log_{2}{\frac{1}{\frac{1}{4}}} \ = \ \log_{2}{4}$ = 2|
|C|$\frac{1}{8}$|$\log_{2}{\frac{1}{\frac{1}{8}}} \ = \ \log_{2}{8}$ = 3|
|D|$\frac{1}{8}$|$\log_{2}{\frac{1}{\frac{1}{8}}} \ = \ \log_{2}{8}$ = 3|

<br>

이를 엔트로피 공식에 대입하면 

$\frac{1}{2} · 1 + \frac{1}{4} · 2  + \frac{1}{8} · 3 + \frac{1}{8} · 3 = 1.75$ 

즉 정보 엔트로피와 평균 질문 개수는 같다. 따라서 우리가 어떤 집단의 확률 분포를 알고 있다면 하나의 대상을 구분하기 위해 필요한 질문 수(=bit)를 계산할 수 있다. 그리고 정보 엔트로피 공식으로 얻은 값이 곧 질문 개수이다.



<br><br>
2부에서는 평균 질문 개수가 정보 엔트로피라는 사실을 바탕으로 정보 엔트로피의 의미에 대해서 이야기하겠다. 

