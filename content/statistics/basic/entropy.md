---
title: "정보 엔트로피"
date: 2022-03-06T13:47:07+09:00
image: "images/home/statistic.png"
Tags: ['엔트로피','Entropy']
draft: true
---
<br>
<!-- 
정보 Entropy는 집단 내 요소의 확률 분포를 알고 있는 상태에서, 새로운 요소를 예측하기 위해 필요한 질문 개수를 알려준다. -->

<br>


**정보 Entropy 공식**

<br>

{{< center >}}$H \ = \ E(x) = \  -\sum p(x) · \log_{2} p(x) $
{{< /center >}}

<br><br>

정보 엔트로피를 배울때면 엔트로피라는 단어와 공식 내 $\log$ 로그로 인해 어렵게 느껴진다. 하지만 공식을 보면 고등학교 확률과 통계에서 봤던 기댓값 공식과 다르지 않다. 정보 엔트로피는 평균 정보량을 구하는 공식일 뿐이다. 

우리가 정보 엔트로피를 배울 때 가장 혼란스러운 이유는 공식이 여러 관점에서 다양하게 해석 가능하기 때문이다. 자연과학 용어인 엔트로피와 공식이 같다는 이유로 정보 엔트로피라는 용어를 사용된다는 것만 보더라도, 이 공식 하나가 세상의 수많은 현상을 설명할 수 있는 가능성을 내포하고 있음을 알 수 있다.

 <!-- 자연과학 용어인 엔트로피와 공식이 같기에 정보 엔트로피라고 이름 불리는 것만봐도 단순한 공식이 세상 돌아가는 여러가지 현상을 설명할 수 있는 단순하지만 강력한 공식이라는 것을 알 수 있다. -->

따라서 사람마다 정보 엔트로피를 어떻게 이해하느냐에 따라 해석이 다양해지게 된다. 예로들어 엔트로피를 현상에 대한 Surprise를 찾는 방법으로 바라보는 경우이다. 

<br>


**Surprise 공식**

$Surprise = \frac{1}{p(x)}$

<br>

Surprise는 발생 확률의 역수를 의미한다. 어떤 현상이 발생 확률이 1/8 이라면 해당 현상의 surprise는 8이 된다. Surprise가 높다는 말은 그만큼 발생 가능성이 낮다는 뜻이므로 통계적으로 많은 정보량을 가지고 있음을 보여준다. Surprise라는 개념을 더 알고싶거나 이를 활용해 정보 엔트로피를 이해하고 싶다면 [StatQuest youtube](https://www.youtube.com/watch?v=YtebGVx-Fxw)를 추천한다.

<br>

이 글은 정보 엔트로피를 보다 직관적인 방법으로 이해하고자 한다. 특히 엔트로피 공식이 어떻게 만들어졌는지, 공식으로 얻은 값이 단순한 숫자인지 아니면 의미를 가지고 있는지 파헤칠 예정이다. 엔트로피 공식이 가지는 의미를 이해한다면 파생되는 여러 의미를, 예로들어 ??, 이해를 바탕으로 받아들일 수 있을거라 생각한다. 

<br>

## 기댓값 | `Expected Value`

앞서 말했듯 정보 엔트로피는 정보량의 기댓값이다. 이번 장에선 기댓값이 무엇인지, 정보 엔트로피가 어떻게 기댓값으로 표현되는지 알아보겠다.

<br>

아래 차트는 주사위 2개를 던진 값의 합에 따라 나올 수 있는 확률을 보여준다. x축은 주사위의 합을 y축은 합이 나올 확률이 표시되어있다. 주사위 2개를 던져 나온 값의 합계의 이산확률분포를 통해 기댓값을 계산해보자.


<br>

![확률분포](/statistics/basic/dice.png) 
출처 위키피디아 이산확률분포

<br>

**기댓값 공식**

{{< center >}} $E(x) = \sum \ p(x) \ ·x \quad \quad $ {{< /center >}}  

<br><br>


기댓값은 확률변수가 취할 수 있는 모든 값들의 평균이다. x는 주사위의 합(2~12)을 의미하고 p(x)는 이러한 합이 나올 확률을 의미한다. 주사위 합이 7이 될 확률은 6/36이다. 그러면 기댓값 공식을 활용해 실제로 주사위 2개의 합에 대한 기댓값을 구해보자.

<br>

 {{< center >}} $E(x) \ = \  2·\frac{1}{36} + 3·\frac{2}{36} + 4·\frac{3}{36} + \ \cdots \ + 11·\frac{2}{36} + 12·\frac{1}{36} = \frac{252}{36}  \ = \ 7 $
{{< /center >}}
<br><br>

공식을 통해 계산된 기댓값은 7이다. 주사위를 무한번 던지면 나온 값들은 평균 7을 가진다는 의미이다. 기댓값을 매번 나온 주사위 값이 7에 수렴한다는 의미로 해석해서는 안된다. 기댓값은 주사위를 던져 나온 값을 모두 합한 뒤 던진 횟수로 나눈 것이므로 한 번 던질때 마다 평균 7정도의 값을 가진다는 의미로 받아드려야한다. 
100번 주사위를 던질때 얻는 값을 알고 싶다면 기댓값을 던진 횟수만큼 곱하면 알 수 있다.

 기댓값은 우리가 일상에서 자주 사용하는 평균과 같은 의미로 활용된다. 다만 기댓값은 구하고자 하는 대상이 확률 변수일 뿐이다.

<br>

<!-- 기댓값에서 짚고넘어가야 할 사실은 기댓값을 구하기 위해서는 특정 집단의 이산확률분포를 안다는 것을 전제해야한다. 정보 엔트로피도 정보량의 기댓값을 계산하는 공식이므로 구하고자 하는 집단의 확률분포를 필요로한다는 사실을 염두에 두자. -->


<br><br>

## 기댓값과 정보 엔트로피

이번엔 정보 엔트로피가 어째서 정보량의 기댓값인지 설명하겠다. 공식만 약간 변경하면 정보 엔트로피 공식과 기댓갑 공식이 다르지 않음을 알 수 있다. 


**정보 Entropy 공식**

<br>

{{< center >}}$H \ = \ E(x) = \  -\sum p(x) · \log_{2} p(x) $
{{< /center >}}

<br><br>

$\sum$ 앞에 있는 마이너스( - )를 $log_{2}{p(x)}$ 앞으로 옮기자. 그러면 정보량 공식인 $ -\log_{2}{p(x)}$ 이 나온다. 그 다음  $ -\log_{2}{p(x)}$ 을 x로 치환하자. 그러면 기댓값 공식인 $E(x) = \sum \ p(x) \ ·x  $이 된다.



<br>

**변경 된 엔트로피 공식**

<br>

{{< center >}}$ H \ =  \ \sum p(x)\ · -\log_{2}{p(x)} \ = \ \sum \ p(x) \ ·x ${{< /center >}}

<br><br>

$x$는 정보량을, $p(x)$는 정보량의 확률을 의미한다. 2개 주사위를 굴려 나온 합에 대한 확률분포로 정보량을 계산해보자. 미리 알고 있는 확률을 이용해 정보량을 구한뒤 계산하면 어렵지 않게 정보량을 구할 수 있다.

|값|$p(x)$|$-\log_{2}{p(x)}$|
|:--:|:----:|:-----------------------:|
|2|$\frac{1}{36}$|$-\log_{2}{\frac{1}{36}} \ = \ \log_{2}{36}$ = 3.584|
|3|$\frac{2}{36}$|$-\log_{2}{\frac{2}{36}} \ = \ \log_{2}{18}$ = 4.169|
|4|$\frac{3}{36}$|$-\log_{2}{\frac{3}{36}} \ = \ \log_{2}{12}$ = 5.169|
|$\cdots$|$\cdots$|$\cdots$|
|10|$\frac{3}{36}$|$-\log_{2}{\frac{3}{36}} \ = \ \log_{2}{12}$ = 5.169 |
|11|$\frac{2}{36}$|$-\log_{2}{\frac{2}{36}} \ = \ \log_{2}{18}$ = 4.169|
|12|$\frac{1}{36}$|$-\log_{2}{\frac{1}{36}} \ = \ \log_{2}{36}$ = 3.584|

<br>


**주사위 합에 대한 정보 엔트로피**

<br>

 {{< center >}} $E(x) \ = \  3.584·\frac{1}{36} + 4.169·\frac{2}{36}  + \ \cdots \  + 4.169·\frac{2}{36} +  3.584·\frac{1}{36} = 3.274 $
{{< /center >}}

<br><br>

정보 엔트로피 공식을 통해 정보량의 기댓값을 구했다. 아직 정보량이 무엇인지 모르기 때문에 이렇게 구한 값은 단순히 주사위 합에 대한 정보량 기댓값이라는 내용 말고는 딱히 의미가 없다. 지금까지 정보 엔트로피가 정보량의 기댓값이라는 사실과 정보 엔트로피를 구하는 방법을 배웠으니 이제는 정보량이 무엇을 의미하는지 살펴보겠다.




<br><br>

## 정보량

**정보량 공식**

<br>

{{< center >}}$I(x) = -\log_{2}{p(x)} ${{< /center >}}

<br><br>

정보량이라는 단어는 정보 엔트로피와 다르게 친숙한 단어처럼 들린다. 단어 뜻 자체가 정보 + 양이라는 의미이다보니 정보량은 정보를 수치화 한 것이라는 생각이 자연스럽게 든다. 

처음 정보량을 접할 때 단어의 친숙함 때문인지 제대로 된 의미를 이해하는데 어려움이 있었다. 우리가 일상에서 흔히 쓰는 용어인 정보는 기본적으로 많으면 좋다는 인식이 깔려있기 때문이었다. 최근 우크라이나 - 러시아 전쟁을 보더라도 미국이 제공하는 막대한 정보량 덕분에 우크라이나가 선전하고 있다는 평이 나올정도로 정보는 중요하고 정보량이 많으면 좋다는 인식이 있다. 이뿐만 아니라 우리는 현재 정보의 홍수 속에서 빅데이터를 통해 새로운 가치를 창출하는 시대에 살고 있다. 정보가 많으면 좋다는 이러한 인식은 많은 사람들이 가지고 있으리라 생각한다.

하지만 우리가 배우고자 하는 정보량은 이러한 선입견을 가진 상태로 바라본다면 이해하는데 어려움을 겪을 수 있다. 실제로 일부 학자들은 정보 이론의 한계 중 하나로 정보 개념에 대해 혼란을 발생시켰다는 점을 꼽기도 한다.

우리가 상식적으로 알고 있는 정보의 의미를 뒤로 한체 정보량이라는 의미를 이해해보자. 여기서 정보량이라는 것은 단어가 됐던, 숫자가 됐던 어떤 기호가 됐건 하나의 대상을 다른 대상과 명확하게 구분할 수 있는 최소한의 크기를 말한다. 

알파벳은 26개의 서로다른 요소로 구성된 하나의 집합이다. A-Z는 집합 내 하나씩만 존재한다. 알파벳 10개를 보내는 그러므로 정보량은 대상을 구분함에 있어서 최소한 비용으로 드는데 필요한 값이 얼마인지에 관심이 많다. 

내가 지금 막히는게 뭐지? 

앞 내용과 뒷 내용이 연결되지 않음.

어째서 2진수를 쓰는거지?

대상을 구분한다고 헀는데 왜 구분하는거지? 통신한다고 할때 1948년도에도 이미 모스부호가 있으니까 그거로 하면 되지않나. 




 철수와 영희가 1~8 사이 숫자 3개를 맞추는 게임을 하고 있다고 생각해보자. 숫자는 1~8 중 랜덤으로 하나가 생성되며  철수는 생성된 숫자를 알고 있다. 영희는 철수에게 질문을 해서 생성된 숫자가 무엇인지를 알아내야 한다. 이때 철수는 영희의 질문에 네 또는 아니오로만 대답해야한다. 영희가 숫자를 맞추기 위해서 택해야 하는 최선의 전략은 무엇일까?

단순히 숫자가 1인지, 2인지 ... 8인지 순차적으로 물어보는 전략을 택한다면 어떻게 될까? 운이 좋다면 한 번에 맞출 수 있지만 운이 좋지 않다면 최대 8번만에 맞출 수 있다. 모 아니면 도 전략이다. 

![onetwo](/statistics/basic/onetwo.png)

여기서 B가 택해야 하는 전략은 이진탐색이다. 이진탐색 은 선택지를 1/2씩 줄여나가는 방법이다. 1~8의 중앙값(median) 4를 기준으로 정답이 4보다 큰지, 4보다 작은지를 A에게 물어보는 것이다. 이런 방식은 선택지를 절반씩 줄여나가기 때문에 매우 효과적으로 대상을 줄여나갈 수 있다.

<그림>

위 그림은 이진 탐색 전략을 택할 때 문제의 정답에 따라 필요한 질문 수를 보여준다. 답이 어떤수인지 상관 없이 B는 3번만 물어보면 모든 문제를 해결 할 수 있다. 

이번에는 0~9 중 하나를 맞추는 경우라 생각해보자. 0~9를 맞출 때는 3~4번 사이면 대부분 해결된다. 그중 3번만에 정답을 맞출 확률이 4번만에 맞출 확률보다 월등히 높다. 이러한 비율은 0~9 문제를 평균 3.x번 질문만에 맞출 수 있다고 생각할 수 있다.

마지막으로 대문자 알파벳을 맞추는 문제를 생각해보자. 알파벳은 A~Z까지 총 26개가 있다. 5번만에 맞출 경우는 20번 4번만에 맞출 경우는 총 6번으로 평균 4.76번이면 맞출 수 있다. 

B가 맞춰야 하는 값의 범위가 8-> 10 -> 26개로 점점 커졌음에도 불구하고 B가 물어봐야 하는 질문 개수는 3개 -> 3.x개 -> 4.76개로 그다지 늘지 않는다. 이 말은 이진탐색 전략이 상당히 효과적인 전략이라는 점을 알 수 있다.

가장 효율적인 전략인 이진탐색은 대상을 1/2씩 줄여나간다. 범위가 8개인 대상을 4개로 2개로 1개로 줄여나간다. 16개인 대상은 8개 4개 2개 1개로 줄여나간다. 

<8과 16 그림>

우리가 여기서 알 수 있는 사실은 $2^{평균 질문개수} = 문제범위$ 라는 것이다. 8과 16 범위에 따라 질문 3번만에 답을 구하는 확률과 질문 4번만에 답을 구하는 확률이 달라짐을 알 수 있다. 8에서 16으로 증가함에 따라 필요한 평균 질문 개수도 증가한다. 이는 당연히 3번 질문해야하는 경우는 8개에서 0개로 감소하는 반면 4번 질문해야하는 경우는 0개에서 16개로 증가하기 때문이다. 

&2^평균질문개수 = 문제범위$가 성립됨을 경험적으로 알게 됐으니 공식을 약간 수정해보자. 우리가 문제범위를 알고 있다면 하나를 맞추기 위해 필요한 평균 질문개수가 몇개인지를 알 수 있다. 

${평균질문개수} = log_{2}{문제범위}$ 식은 문제범위가 몇개인지 안다면 평균질문개수가 어떻게 될지 쉽게 알 수 있다. 여기서 놓치지 말아야할 사실은 평균 질문 개수는 대상 하나를 식별하기 위해 필요한 평균 질문 수를 의미한다. 

알파벳 하나를 식별하기 위해서는 $log_{2}{26} \approx 4.7004$개의 질문이 필요하다. 알파벳 10,000개를 알아내야한다면 4.7004*10000이 되므로 47004번 질문하면 된다.

이를 데이터 단위로 표현하면 알파벳 10,000개를 식별하기 위해서는 47004비트만 있으면 된다는 말이기도 하다.


<br>

**$log_{2}$문제범위 로 평균 질문 개수를 구할 수 있는지 검증하기**

사실 지금 설명하고자 하는 내용이 내가 엔트로피에 대한 글을 작성하게 된 동기였다. 어떻게 $log_{2}{문제범위}$라는 식이 평균 질문개수를 바로 계산할 수 있는지 의문이 들었다. 

이런 의문을 해결하고자, 알파벳을 맞추기 위해 필요한 질문 개수가  $log_{2}{문제범위}$로 얻은 값과 아래 그림처럼 질문 개수와 확률을 통해 얻은 값이 같은지 확인했다. 

그 결과 $log_{2}(26)$는 4.7004이고 4와 5가 나오는 비율로 계산한 결과는 4.76이었다. 이 두 값은 유사하다고 할 수 있지만 알파벳 1000개를 알아내는 질문인 경우 약 500여개 정도 차이나기 때문에 우연의 일치가 아닌가 하는 의문이 생겼다. 한편으로는 내가 정보엔트로피를 제대로 이해하고 있는게 맞나 싶은 강력한 의문을 가지게 됐다. 

결론부터 말하면 문제 푸는 횟수가 증가 할수록 $log_{2}(26)$ 값에 수렴한다.

아래 코드를 활용해 알파벳을 100만번 알아내는 경우를 실험해봤다. 횟수가 증가할수록 알파벳 하나를 알아내기 위해 필요한 질문 개수가 4.7004에 수렴하는 것을 볼 수 있다.

<br>

![output](statistics/basic/output.png)
{{< center >}} Alphabet 문제를 1,000,000번 실행한 결과{{< /center >}}

<br>

나와 같은 궁금함을 가진 사람들을 위해 작성한 코드를 공개한다. 

<br>

```python
import math
import pandas as pd
import numpy as np

k = []

for j in range(1000000) :  
    alpha = list(range(26)) ## Alphabet을 0~25로 치환한다고 생각
    target = np.random.randint(26)  ## 0~25 중 랜덤으로 값 생성

    i = 0 
    while i < 10 :
        median = (alpha[0] + alpha[-1])// 2

        if len(alpha) == 2 or(median == target and len(alpha) == 3):
            k.append(i+1) ## i = 물어본 질문 수 
            break
        
        ## 값이 짝수개인 리스트를 나눌 때
        elif len(alpha)%2 == 0 :
            if target > median :
                alpha = list(range(median+1, alpha[-1]+1))
                # print(f" 짝 T >= M --- median : {median} --- alpha : {alpha}")  

            if target <= median : 
                alpha = list(range(alpha[0], median+1))
                # print(f" 짝 T < M --- median : {median} --- alpha : {alpha}")

        ## 값이 홀수개인 리스트를 나눌 때
        elif len(alpha)%2 == 1 : 
            if target >= median :
                alpha = list(range(median, alpha[-1]+1))
                # print(f" 홀 T > M --- median : {median} --- alpha : {alpha}")

            if target < median : 
                alpha = list(range(alpha[0], median))
                # print(f" 홀 T < M --- median : {median} --- alpha : {alpha}")
                
        i += 1



result = pd.Series(k)
mean = result.mean()  ## 평균
val_k = result.value_counts() ## 빈도수

print('평균 : ', round(mean,6))
print('log_2(26) :', round(math.log2(26),6))
print('차이 :', round(mean - math.log2(26),6))
print('-'*10)
print('-'*10)
print(val_k)
```

<br><br>

**정보량과 평균질문개수**

이 글의 목적이 엔트로피 공식을 이해하는 것이므로 지금까지 배운 내용을 다시 정리하여 어떻게 엔트로피와 관련이 있는지 알아보겠다. 우리는 정보 엔트로피가 X에 대한 기대값이라는 사실을 알았다. X라 함은 $\log_{2}({\frac{1}{p(x)}})$ 공식을 말한다. 그리고 $\log_{2}({\frac{1}{p(x)}})$ 공식은 정보량이라는 이름을 가지고 있다. 우리가 1~8, 0~9, 알파벳 문제를 풀면서 알게된 공식은 평균질문개수 = $log_{2}$문제범위 였다. 

정보량이라는 개념은 확률을 통해 구한 값인 반면 평균질문 개수는 문제범위를 가지고 구한 값이므로 같은 두 공식이 닮긴 했는데 같인 값인지 애매하다. 이 내용의 처음으로 다시 돌아가보자. A와 B는 랜덤으로 생성된 값을 가지고 게임을 하고 있다. A는 알파벳 A~Z 중 랜덤으로 하나의 값을 고르고 B는 질문을 통해 값을 맞춰야한다. 

여기서 우리는 너무 당연히 여기다보니 고민조차 하지 않는 조건이 하나 있다. 다른 조건이 주어지지 않는 한 A~Z를 무작위로 무수히 선택한다면 개별 값의 빈도수가 모두 같으리라 생각한다. 이를 다르게 표현하자면 개별 값들이 선택될 확률이 일정하다는 사실이다. 즉 A-Z의 개별 값은 1/26의 확률을 가진다.

테이블

|값|확률|
|:------:|:---:|
|A|$\frac{1}{26}$|
|B|$\frac{1}{26}$|
|$\vdots$|$\vdots$|
|Y|$\frac{1}{26}$|
|Z|$\frac{1}{26}$|

A~Z까지 확률이 같으니 대표로 A의 정보량을 구해보자.  $p(x)=\frac{1}{26}$ 이므로 $\log_{2}({\frac{1}{\frac{1}{26}}})$ = $log_{2}(26)$ 이 된다.

흥미롭게도 정보량과 평균질문개수가 동일한 식이다. 따라서 정보량이라는 이름을 가진  $\log_{2}({\frac{1}{p(x)}})$은 대상 하나를 식별하는데 필요한 평균질문개수를 의미한다. 

<br><br>


## 엔트로피 공식 이해하기
지금까지 엔트로피 공식이 X에 대한 기댓값이라는 사실과 기댓값 X로 치환했던 존재인 $\log_{2}({\frac{1}{p(x)}})$가 평균질문개수라는 사실을 설명했다. 

이렇게 알게 된 두 사실을 조합하면 정보 엔트로피 공식의 의미를 알 수 있다. 결론을 도출하기에 앞서 앞에서 계속해서 설명한 예시인 알파벳 문제의 정보 엔트로피를 구해보자.

<br>

**정보 Entropy 공식**

{{< center >}}$ H \ = \ -\sum p(x)\ · \log_{2}{p(x)} \ = \ \sum p(x)\ · \log_{2}({\frac{1}{p(x)}})  ${{< /center >}}

<br><br>

A~Z는 발생할 확률이 동일하다. 확률 $p(x)=\frac{1}{26}$ 이고 정보량 $I(x) = log_{2}(26)$이다. 따라서 $ H = E(x) = \sum \frac{1}{26} \ · \ log_{2}(26) $이다.  

A~Z 값이 26개이므로 아래와 같은 식이 성립된다. 
<br>

$\frac{1}{26} \ · \ log_{2}(26) · 26 = log_{2}(26) \approx 4.70004$

정보 엔트로피를 계산하니 정보량과 값이 같다. 돌고돌아보니 결국 정보 엔트로피, 정보량 평균 질문 개수는 동의어에 불과하다..... 과연 그럴까?

<br><br>

### 발생할 확률이 동일하지 않은 경우
지금까지 우리는 발생할 확률이 동일한 경우를 예시로 활용했다. 예시가 다르다고 지금까지 배웠던 개념이 달라지는건 없다. 정보 엔트로피는 X에 대한 기대값이고 정보량인 X는 평균 질문개수다.

<!-- 
좀 더 현실적인 예시를 들어보자. 아래 테이블은 전세계 노트북 시장 점유율을 보여준다. 주어진 값은 상위 5개 기업과 나머지 기업을 합친 others로 총 6개이다. A와 B가 이번엔 노트북 회사 이름 맞추기 게임을 하고 있다.. A는 기업의 점유율에 맞게 회사를 선택한다. B는 문제 하나를 맞추기 위해 평균 몇 번을 질문 해야할까? 

|회사|점유율|
|:------:|:---:|
|Lenovo|$30%$|
|HP|$20%$|
|DELL|$15%$|
|APPLE|$10%$|
|ACER|$5%$|
|Others|$20%$|

이전 게임에서 최고의 전략으로 여겨졌던 이진탐색을 사용해보자.

정보량은 구할 수 있는데 트리는 어떻게 구현해야하는지 모르겠다. -->


이번엔 난이도를 높혀서 A~D의 발생확률이 다른 상황에서 문제를 맞춰야 한다. A~D가 발생할 확률은 아래 테이블과 같다. 

|값|확률|
|:------:|:---:|
|A|$0.5$|
|B|$0.25$|
|C|$0.125$|
|D|$0.125$|

앞선 예시처럼 단순히 절반을 나누는 방법을 사용해보자. 아래 그림과 같은 구조를 가지게 되며 정보엔트로피 = 정보량 = 평균 질문 개수 = 2가 된다.
그림

하지만 개별 값의 확률 분포를 고려하지 않고 만들었기에 최적의 전략은 아니다.
차라리 이런 모양의 트리를 만든다면 어떻게 될까? A가 선택될 학률이 50%이니 가장 먼저 A를 물어보는게 가능성을 더 높혀주기 때문이다. 이와 마찬가지로 B가 C 또는 D보다 발생 확률이 2배 높으니 B를 물어보는게 좋은 선택이 된다. 완성된 트리는 다소 불균형해 보이지만 실제로는 효과가 좋다. 


이 전략이 효과적이라는 사실을 알았다. 이제는 B가 이 방법을 택해서 문제를 해결한다면 평균 몇번 질문을 필요로하는지 계산해보자. 단순히 반반씩 나눈 전략에 비해 얼마나 질문 수를 줄일 수 있을지 비교하기 위함이다.

A와 B는 40번의 문제 맞추기 게임을 했다. 문제 비율은 확률과 똑같이 발생했다. A는 40번 중 20번, B는 10번 C와 D는 각각 5번 발생했다.  


|값|빈도수|질문 개수|
|:--:|:----:|:--:|
|A|20회|1번|
|B|10회|2번|
|C|5회|3번|
|D|5회|3번|
총합|40회||

빈도수와 질문 개수를 알았으니 이 방법을 사용한다면 평균적으로 사용하는 질문 개수를 알 수 있다. 이 방법을 활용하면 평균 1.75번 질문이 필요하다는 사실을 알수 있다.

<br>

{{< center >}}$\frac{20·1+10·2+5·3+5·3}{40} \ = \ \frac{20}{40} · 1 + \frac{10}{40} · 2  + \frac{5}{40} · 3 + \frac{5}{40} · 3 \ = \ 1.75$ {{< /center >}}

<br><br>




### 정보 엔트로피와 평균 질문 개수
앞선 설명에서 알파벳 문제 처럼 개별 값들의 확률이 동일할 경우, 정보 엔트로피 = 개별 값의 정보량 = 평균 질문 개수라는 사실을 확인했다. 개별 값이 발생할 확률이 다른 경우에도 정보 엔트로피 = 개별 값의 정보량 = 평균 질문 개수가 사실일까? 발생 확률이 다른 A~D 문제의 정보 엔트로피를 구해보자.

<br>

**정보 엔트로피 공식**

|값|$p(x)$|$\log_{2}{p(x)}$|
|:--:|:----:|:-----------------------:|
|A|$\frac{1}{2}$|$\log_{2}{\frac{1}{\frac{1}{2}}} \ = \ \log_{2}{2}$ = 1|
|B|$\frac{1}{4}$|$\log_{2}{\frac{1}{\frac{1}{4}}} \ = \ \log_{2}{4}$ = 2|
|C|$\frac{1}{8}$|$\log_{2}{\frac{1}{\frac{1}{8}}} \ = \ \log_{2}{8}$ = 3|
|D|$\frac{1}{8}$|$\log_{2}{\frac{1}{\frac{1}{8}}} \ = \ \log_{2}{8}$ = 3|

<br>

이를 엔트로피 공식에 대입하면 

$\frac{1}{2} · 1 + \frac{1}{4} · 2  + \frac{1}{8} · 3 + \frac{1}{8} · 3 = 1.75$ 

즉 정보 엔트로피와 평균 질문 개수는 같다. 따라서 우리가 어떤 집단의 확률분포를 알고있다면 하나의 대상을 구분하기 위해 필요한 질문 수(=bit)를 계산할 수 있다. 그리고 정보 엔트로피 공식으로 얻은 값이 곧 질문 개수이다.



<br><br>
2부에서는 평균 질문 개수가 정보 엔트로피라는 사실을 바탕으로 정보 엔트로피의 의미에 대해서 이야기하겠다. 

